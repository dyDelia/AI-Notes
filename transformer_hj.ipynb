{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0da537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627d740",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "    - embed_size就是d_k， Q,K,V都是 batch, n_, dimension 这样的组成的话，embed_size就是Q.size(-1), 也就是dimension, 即 d_k\n",
    "    - 这里mask，是把mask为0的地方全部取负无穷， -inf;softmax之后这些位置会变成0\n",
    "    - softmax的dim = -1，意味着取最后一维做softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d52d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q,K,V,mask=None):\n",
    "    \n",
    "    # scale的sqrt dk, d_k = d_q\n",
    "    embed_size = Q.size(-1)\n",
    "\n",
    "    score = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(embed_size)\n",
    "\n",
    "    if mask is not None:\n",
    "        score = score.masked_fill(mask==0, float('-inf'))\n",
    "    attention = F.softmax(score,dim = -1)\n",
    "\n",
    "    output = torch.matmul(attention, V)\n",
    "\n",
    "    return attention, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba873d",
   "metadata": {},
   "source": [
    "## 单头注意力机制，这里有输入的 线性变换\n",
    "- nn.Linear(in_features,out_features)是线性变化层，定义了一个输入维度in_feature,输出维度out_feature的线性变换层\n",
    "- 这里的线性变换是 \n",
    "$$y = xW^t + b $$\n",
    "\n",
    "- 也就是 这里单头注意力机制，就是将输入的序列，首先进行线性变换成查询矩阵，key矩阵，value矩阵，然后进行scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf55365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        单头注意力机制\n",
    "\n",
    "        参数embed_size, 输入序列的嵌入维度\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        # 定义线性层，用于生成查询、键和值矩阵\n",
    "        self.w_q = nn.Linear(embed_size, embed_size)\n",
    "        self.w_k = nn.Linear(embed_size, embed_size)\n",
    "        self.w_v = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "\n",
    "        参数：\n",
    "        q:查询矩阵， batch_size, seq_len_q, embed_size\n",
    "        k: 键矩阵，batch_size, seq_len_k, embed_size\n",
    "        v: 值矩阵，batch_size, seq_len_v, embed_size\n",
    "\n",
    "        return:\n",
    "            output:注意力加权之后的输出\n",
    "            atten:注意力权重\n",
    "\n",
    "        \"\"\"\n",
    "        Q = self.w_q(q)\n",
    "        K = self.w_k(k)\n",
    "        V  = self.w_v(v)\n",
    "\n",
    "        attention,out = scaled_dot_product_attention(Q,K,V, mask)\n",
    "        return attention,out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526e737",
   "metadata": {},
   "source": [
    "## 掩码机制\n",
    "- 当需要掩藏未来的词，就需要mask\n",
    "- 发生在softmax计算之前\n",
    "    - 首先，$$QK^T$$,这里的计算还是需要所有的值\n",
    "    - 在计算score的时候，所有位置保留\n",
    "    - 在计算softmax之前排除，这样就可以让模型看不到未来词，来进行下一个词的预测\n",
    "- 这里是look ahead mask， 用于decoder\n",
    "\n",
    "- 如果是padding mask,是处理填充的符，在encoder,decoder里面都要mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e666219",
   "metadata": {},
   "source": [
    "![mask](./images/mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062fac4",
   "metadata": {},
   "source": [
    "## 自注意力机制，交叉注意力机制\n",
    "- 自注意力机制的Q, K, V同源\n",
    "- transformer的decoder交叉注意力机制，Q 来自上一个decoder block， K,V来自encoder的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d770140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "\n",
    "        \"\"\"\n",
    "        自注意力（Self-Attention）机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列的嵌入维度（每个向量的特征维度）。\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention = Attention(embed_size)\n",
    "    \n",
    "    def forward(self,x, mask=None):\n",
    "\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 输入序列 (batch_size, seq_len, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "            out: 自注意力加权后的输出 (batch_size, seq_len, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        out, attention = self.attention(x,x,x,mask)\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673176c5",
   "metadata": {},
   "source": [
    "## 交叉注意力机制\n",
    "\n",
    "$$Q = X_{decoder} W^Q $$\n",
    "$$ K = X_{encoder} W^K $$\n",
    "$$V = X_{encoder} W^V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.attention = SelfAttention(embed_size)\n",
    "\n",
    "    def forward(self, q, kv, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            query: 查询矩阵的输入 (batch_size, seq_len_q, embed_size)\n",
    "            kv: 键和值矩阵的输入 (batch_size, seq_len_kv, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出 (batch_size, seq_len_q, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len_kv, seq_len_kv)\n",
    "        \"\"\"\n",
    "        # 在交叉注意力机制中，q 和 k, v 不同\n",
    "        # q 来自解码器，k 和 v 来自编码器（观察模型架构图）\n",
    "        out,atten = self.attention(q,kv,kv,mask = mask)\n",
    "\n",
    "        return out,atten\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd7245",
   "metadata": {},
   "source": [
    "## 多头注意力机制\n",
    "- 用多个注意力头，并行地关注输入数据在不同维度上的依赖关系\n",
    "- 多个头，每个头有独立的线性变化，\n",
    "    - $head_i =  Attention(QW^Q_{i},KW^K_{i},VW^V_{i})$\n",
    "    - 最后沿最后一维拼接，通过线性变换矩阵$W^Q$映射会原始嵌入维度embed_size\n",
    "    -  $MultiHead(Q,K,V) = Concat(head_1,...m head_h)W^o$\n",
    "- 映射回原始维度的主要目的是为了实现残差连接（Residual Connection）,张量维度需要匹配\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f3658",
   "metadata": {},
   "source": [
    "###\n",
    "nn.ModuleList([nn.Linear(embed_size, embed_size) for _ in range(num_heads)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0be675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个凭借直觉的写法\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        \"\"\"\n",
    "        多头注意力机制\n",
    "        参数：\n",
    "        embed size\n",
    "        num_heads\n",
    "\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.w_q = nn.ModuleList([nn.Linear(embed_size, embed_size) for _ in range(num_heads)])\n",
    "        self.w_k = nn.ModuleList([nn.Linear(embed_size, embed_size) for _ in range(num_heads)])\n",
    "        self.w_v = nn.ModuleList([nn.Linear(embed_size, embed_size) for _ in range(num_heads)])\n",
    "\n",
    "        self.fc_out = nn.Linear(num_heads*embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, q,k,v,mask = None):\n",
    "        batch_size = q.size(0)\n",
    "        multi_head_outputs = []\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            Q = self.w_q[i](q)\n",
    "            K = self.w_k[i](k)\n",
    "            V = self.w_v[i](v)\n",
    "\n",
    "            attention, _ = scaled_dot_product_attention(Q,K,V,mask)\n",
    "            multi_head_outputs.append(attention)\n",
    "        \n",
    "        concat_out = torch.cat(multi_head_outputs,dim=-1)\n",
    "        # 把多个张量沿某个维度拼接起来, 这里就是沿最后一个维度，embed_size拼接，所以最后拼接了num_head次\n",
    "        out = self.fc_out(concat_out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    # 每个头都用独立的线性变换，且维度是embed_size,模型的参数量非常大，性能的提升会不会和参数量变大有关\n",
    "    # 如果想评估性能提升的关键之处\n",
    "    # 1.把单头注意力的参数量提高到多头的量，也就是说 原本维度是d*d,现在维度是d*(d*h)\n",
    "    # 2.把每个多头的维度都降低，到跟单头一样，也就是每个线性层是 d*(d/num_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb379b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将多头的每个头维度控制为 embed_size/num_head\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embed_size % num_heads == 0, \"embed_size 必须能被 num_heads 整除。\"\n",
    "        #强制检查这个条件是否符合，否则报出错误\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads  # 每个头的维度\n",
    "\n",
    "        # nn.linear(输入features，输出的features)\n",
    "        self.w_q = nn.ModuleList([nn.Linear(embed_size, self.head_dim) for _ in range(num_heads)])\n",
    "        self.w_k = nn.ModuleList([nn.Linear(embed_size, self.head_dim) for _ in range(num_heads)])\n",
    "        self.w_v = nn.ModuleList([nn.Linear(embed_size, self.head_dim) for _ in range(num_heads)])\n",
    "\n",
    "        # 输出线性层，将多头拼接后的输出映射回 embed_size\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "        multi_head_outputs = []\n",
    "\n",
    "        # 针对每个头独立计算 Q, K, V，并执行缩放点积注意力\n",
    "        for i in range(self.num_heads):\n",
    "            Q = self.w_q[i](q)  # (batch_size, seq_len_q, head_dim)\n",
    "            K = self.w_k[i](k)  # (batch_size, seq_len_k, head_dim)\n",
    "            V = self.w_v[i](v)  # (batch_size, seq_len_v, head_dim)\n",
    "\n",
    "            # 执行缩放点积注意力\n",
    "            scaled_attention, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "            multi_head_outputs.append(scaled_attention)\n",
    "\n",
    "        # 将所有头的输出拼接起来\n",
    "        concat_out = torch.cat(multi_head_outputs, dim=-1)  # (batch_size, seq_len_q, embed_size)\n",
    "\n",
    "        # 通过输出线性层\n",
    "        out = self.fc_out(concat_out)  # (batch_size, seq_len_q, embed_size)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# 这个代码使用了for循环逐一计算每个头的线性变换，但是，不是很计算高效\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56d993",
   "metadata": {},
   "source": [
    "## 多头的实现，优雅实现\n",
    "- 原本是为每个头单独创建线性层，现在创造一个共享线性层\n",
    "    ```\n",
    "    ### “共享”的 Q, K, V 线性层\n",
    "    self.w_q = nn.Linear(embed_size, embed_size)\n",
    "    self.w_k = nn.Linear(embed_size, embed_size)\n",
    "    self.w_v = nn.Linear(embed_size, embed_size)\n",
    "    ```\n",
    "- 原本是循环遍历每个层，计算score\n",
    "    - 一次性计算Q，K,V，用reshape, transpose，将矩阵拆分为多头的格式\n",
    "    - 如上，self.w_q（q）是batch, n_q, embed_size\n",
    "        - `Q.reshape(batch, n_q, num_heads, head_dim).transpose(1,2)`\n",
    "        - 拆分了embed_size，之后，将形状转变为batch, num_heads, n_q, head_dim\n",
    "        - 这样做的目的是，后续的每个头的计算是独立的\n",
    "        - 如果不用reshape,用view的话，需要在拼接的时候先用contiguous()，因为view要求输入张量在内存上连续"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62dc11",
   "metadata": {},
   "source": [
    "### 原本的scaled-dot-product attention也需要修改，因为 形状的改变\n",
    "```\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "\t\"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        K: 键矩阵 (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        V: 值矩阵 (batch_size, num_heads, seq_len_v, head_dim)\n",
    "        mask: 掩码矩阵 (1, 1, seq_len_q, seq_len_k) 或 (batch_size, 1, seq_len_q, seq_len_k) 或 (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    ...（操作依旧不变，只需要改注释）\n",
    "    return output, attention_weights    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e820b8e",
   "metadata": {},
   "source": [
    "### 广播机制broadcasting\n",
    "- 用一个较小形状的张量与较大的张量做运算时，只要维度是兼容的，就会自动复制扩展来匹配\n",
    "- 所以 (1, 1, seq_len_q, seq_len_k) 可以自动广播到 (batch_size, num_heads, seq_len_q, seq_len_k)。\n",
    "- 维度广播，因为维度兼容，自动复制扩展到大的维度形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f9ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a0e0cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
